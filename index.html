<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-10550309-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-10550309-8');
</script>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems</title>
        <meta property="og:title" content="phMARL" />
        <meta property="og:image" content="" />
        <meta property="og:url" content="" />
      <style>
code {
  font-family: Consolas,"courier new";
  color: darkcyan;
  background-color: #f1f1f1;
  padding: 2px;
  font-size: 105%;
}
</style>
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems</span>
    
    </center>

    <br><br>
      <table align=center width=900px>
       <tr>
         <td align=center width=900px>
         <center>
         <span style="font-size:20px"><a href="https://eduardosebastianrodriguez.github.io/">Eduardo Sebastian</a>,
             <a href="https://thaipduong.github.io/">Thai Duong</a>,
             <a href="https://natanaso.github.io/">Nikolay Atanasov</a>,
             <a href="https://sites.google.com/unizar.es/eduardo-montijano">Eduardo Montijano</a> and
             <a href="https://webdiis.unizar.es/~csagues/">Carlos Sagues</a>
         </span>
         </center>
         </td>
     </tr>
    </table>

    <br>

    <table align=center width=800px>
       <tr>
        <td align=left width=350px>
        <center>
        <span style="font-size:20px">Departamento de Informatica e Ingenieria de Sistemas, <br> Universidad de Zaragoza<br/></span>
        </center>
        </td>
        <td align=right width=100px>
        <center>
        <span style="font-size:20px"></span>
        </center>
        </td>
        <td align=right width=350px>
        <center>
        <span style="font-size:20px">Department of Electrical and Computer Engineering, <br> University of California, San Diego<br/></span>
        </center>
        </td>
     </tr>
    </table>

    <br>

    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://www.ieee-ras.org/publications/t-ro">Under review at IEEE Transactions on Robotics</a></span>
        </center>
        </td>
     </tr>
    </table>

    <br>

    <table align=center width=500px>
     <tr>
       <td align=center width=100px>
       <center>
       <span style="font-size:20px"><a href="https://arxiv.org/abs/2401.00212">[Paper]</a></span>
       </center>
       </td>
       <td align=center width=100px>
       <center>
       <span style="font-size:20px"><a href="https://github.com/EduardoSebastianRodriguez/phMARL">[Code]</a></span>
       </center>
       </td>
    
   </tr>
  </table>

            <br>
            

            <br>
            We propose a physics-informed reinforcement learning approach able to learn distributed multi-robot control policies that are both scalable and make use 
            of all the available information to each robot. Our approach has three key characteristics. First, it imposes a port-Hamiltonian structure on the policy representation, 
            respecting energy conservation properties of physical robot systems and the networked nature of robot team interactions. 
            Second, it uses self-attention to ensure a sparse policy representation able to handle time-varying information at each robot from the interaction graph. 
            Third, we present a soft actor-critic reinforcement learning algorithm parameterized by our self-attention port-Hamiltonian control policy, 
            which accounts for the correlation among robots during training while overcoming the need of value function factorization. 
            <br><br>

      <hr>
               <!-- <table align=center width=550px> -->
            <table align=center width=900>
             <center><h1>Paper</h1></center>
                <tr>
                    <!--<td width=300px align=left>-->
                    <!-- <a href="https://arxiv.org/abs/2401.00212"> -->
                  <!-- <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./resources/images/paper.png"/></a></td> -->
                  <td><a href="https://arxiv.org/pdf/2401.00212.pdf"><img style="height:300px" src="./resources/thumbnail.png"/></a></td>
                  <td><span style="font-size:14pt">Eduardo Sebastian, Thai Duong, Nikolay Atanasov, Eduardo Montijano and Carlos Sagues<br><br>
                           Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems<br><br>
                  Under review at IEEE Transactions on Robotics, 2024.<br><br>
                      <a href="https://arxiv.org/abs/2401.00212">[pdf]</a> &nbsp; &nbsp;
                   <!--    <a href="./resources/bibtex.txt">[Bibtex]</a> -->
                  <!-- [hosted on <a href="#">arXiv</a>]</a> -->
                    </td>
              </tr>
            </table>

          <hr>
          <table align=center width=550px>
            <table align=center width=800>
             <center><h1>Details and Multimedia</h1></center>
                <tr>
                    <br>
            <br>

                  <table align=center width=1000px>
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/simple_spread_example_modified.png"><img src = "./resources/simple_spread_example_modified.png" width="220px" height="220px"></img></href></a>
                     <a href="./resources/sampling_example_modified.png"><img src = "./resources/sampling_example_modified.png" width="220px" height="220px"></img></href></a>
                     <a href="./resources/reverse_transport_example_modified.png"><img src = "./resources/reverse_transport_example_modified.png" width="220px" height="200px"></img></href></a>
                     <a href="./resources/grassland_example_modified.png"><img src = "./resources/grassland_example_modified.png" width="220px" height="200px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
                    <td width=600px>
                      <center>
                          <span style="font-size:18px"><i>Examples of scenarios addressed by our physics-informed multi-agent reinforcement learning approach. The scenarios cover a wide variety of cooperative/competitive behaviors and levels of coordination complexity. From left to right: (a) robots learn to navigate to landmarks (black dots) while avoiding collisions; (b) robots cooperate to do active LiDAR sampling (orange areas) in an environment with unknown regions of interest (green areas); (c) robots cooperate to transport a squared box (red) towards a desired region (green dot); (d) the robots (blue) collect food (green) while avoiding collisions with attackers (red).</i>
                    </center>
                    </td>
                </tr>
            </table>
            <br><br>

                  
            <table align=center width=1000px>
                <tr>
                    <td width=1000px>
                      <center>
                          <a href="./resources/overall.png"><img src = "./resources/overall.png" width="1000px"></img></href></a><br>
                    </center>
                    </td>
                </tr>
                    <td width=1000px>
                      <center>
                          <span style="font-size:18px"><i> Overview of our physics-informed soft actor-critic multi-agent reinforcement learning approach. The main differences with respect of other actor-critic methods are the following: (a) the actor is the multi-robot network modeled as a port-Hamiltonian system and with a policy given by a self-attention-based IDA-PBC; (b) the replay buffer not only stores action, states and reward, but also the graph structure of the multi-robot network to enforce the desired distributed structure; (c) the reward is global because the actor is the whole multi-robot team and the physics-informed parameterization already conditions the policy on the graph structure of the team; (d) the output of the critic is shared across robots and the policy parameters are the same for all robots, so for the same critic gradient step, n policy gradient steps are taken.</i>
                    </center>
                    </td>
                </tr>
            </table>
                <br><br>
                  
            <table align=center width=1000px>
                <tr>
                    <td width=1000px>
                      <center>
                          <a href="./resources/Architecture.png"><img src = "./resources/Architecture.png" width="1000px"></img></href></a><br>
                    </center>
                    </td>
                </tr>
                    <td width=1000px>
                      <center>
                          <span style="font-size:18px"><i>Physics-informed policy parameterization. At each instant, the robot receives state information from its neighbors, typically associated to a perception or communication radius. Then, three self-attention-based modules, each one associated to a component of the desired closed-loop port-Hamiltonian dynamics, uses the information from the neighbors to compute the parameters of the IDA-PBC policy, which is then used to execute the desired control action.</i>
                    </center>
                    </td>
                </tr>
            </table>
                <br><br>

            
            <table align=center width=1000px>
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/reverse_transport_scalability.png"><img src = "./resources/reverse_transport_scalability.png" width="300px"></img></href></a>
                     <a href="./resources/sampling_scalability.png"><img src = "./resources/sampling_scalability.png" width="300px"></img></href></a>
                     <a href="./resources/navigation_scalability.png"><img src = "./resources/navigation_scalability.png" width="300px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
                    <td width=600px>
                      <center>
                          <span style="font-size:18px"><i>Comparison of the performance of the ablated control policies when we scale the number of robots in deployment. In all the scenarios, our proposed combination of a port-Hamiltonian modeling and self-attention-based neural networks achieves the best cumulative reward without further training the control policy. Each bar displays the mean and standard deviation of the average cumulative reward over 10 evaluation episodes.</i>
                    </center>
                    </td>
                </tr>
            </table>
            <br><br>

    <table align=center width=1000px>
                  <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/legend_new.png"><img src = "./resources/legend_new.png" width="300px"></img></href></a>
                 </span>
                 </center>
                 </td>
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/food_collection_scalability_new.png"><img src = "./resources/food_collection_scalability_new.png" width="300px" height="150px"></img></href></a>
                     <a href="./resources/grassland_scalability_new.png"><img src = "./resources/grassland_scalability_new.png" width="300px" height="150px"></img></href></a>
                     <a href="./resources/adversarial_scalability_new.png"><img src = "./resources/adversarial_scalability_new.png" width="300px" height="150px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
                    <td width=600px>
                      <center>
                          <span style="font-size:18px"><i>Comparison of our proposed physics-informed multi-agent reinforcement learning approach with other state-of-the-art approaches. pH-MARL is only trained with 4 robots and deployed with different team sizes, while the state-of-the-art control policies are trained for each specific number of robots.</i>
                    </center>
                    </td>
                </tr>
            </table>
            <br><br>

<center><h1>Some Qualitative Results</h1></center>          

    <table align=center width=1000px>
      <td width=600px>
                      <center>
                          <span style="font-size:18px"><i>Examples of multi-robot scenarios for different initial conditions and number of robots. It is interesting to see that some aspects of the environment do not scale with the team size, e.g., the size and weight of the box in the reverse transport scenario, the number of hot spots in the sampling scenario or the size of the arena in the navigation scenario. From top to bottom: reverse transport, navigation, sampling, food collection, grassland, adversarial.</i>
                    </center>
                    </td>
                </tr>
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/reverse_transport_4_7_LEMURS.gif"><img src = "./resources/reverse_transport_4_7_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/reverse_transport_8_3_LEMURS.gif"><img src = "./resources/reverse_transport_8_3_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/reverse_transport_12_4_LEMURS.gif"><img src = "./resources/reverse_transport_12_4_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/reverse_transport_16_9_LEMURS.gif"><img src = "./resources/reverse_transport_16_9_LEMURS.gif" width="220px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
                
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/simple_spread_4_9_LEMURS.gif"><img src = "./resources/simple_spread_4_9_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/simple_spread_5_0_LEMURS.gif"><img src = "./resources/simple_spread_5_0_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/simple_spread_6_1_LEMURS.gif"><img src = "./resources/simple_spread_6_1_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/simple_spread_8_0_LEMURS.gif"><img src = "./resources/simple_spread_8_0_LEMURS.gif" width="220px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
      
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/sampling_3_0_LEMURS.gif"><img src = "./resources/sampling_3_0_LEMURS_new.gif" width="220px"></img></href></a>
                     <a href="./resources/sampling_5_3_LEMURS.gif"><img src = "./resources/sampling_5_1_LEMURS_new.gif" width="220px"></img></href></a>
                     <a href="./resources/sampling_7_0_LEMURS.gif"><img src = "./resources/sampling_7_0_LEMURS_new.gif" width="220px"></img></href></a>
                     <a href="./resources/sampling_9_0_LEMURS.gif"><img src = "./resources/sampling_9_0_LEMURS_new.gif" width="220px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
      
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/simple_spread_food_3_2_LEMURS.gif"><img src = "./resources/simple_spread_food_3_2_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/simple_spread_food_6_2_LEMURS.gif"><img src = "./resources/simple_spread_food_6_2_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/simple_spread_food_12_1_LEMURS.gif"><img src = "./resources/simple_spread_food_12_1_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/simple_spread_food_24_6_LEMURS.gif"><img src = "./resources/simple_spread_food_24_6_LEMURS.gif" width="220px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
      
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/grassland_vmas_3_8_LEMURS.gif"><img src = "./resources/grassland_vmas_3_8_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/grassland_vmas_6_2_LEMURS.gif"><img src = "./resources/grassland_vmas_6_2_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/grassland_vmas_12_1_LEMURS.gif"><img src = "./resources/grassland_vmas_12_1_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/grassland_vmas_24_9_LEMURS.gif"><img src = "./resources/grassland_vmas_24_9_LEMURS.gif" width="220px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
      
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/adversarial_vmas_3_2_LEMURS.gif"><img src = "./resources/adversarial_vmas_3_2_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/adversarial_vmas_6_1_LEMURS.gif"><img src = "./resources/adversarial_vmas_6_1_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/adversarial_vmas_12_1_LEMURS.gif"><img src = "./resources/adversarial_vmas_12_1_LEMURS.gif" width="220px"></img></href></a>
                     <a href="./resources/adversarial_vmas_24_0_LEMURS.gif"><img src = "./resources/adversarial_vmas_24_0_LEMURS.gif" width="220px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
            </table>
            <br><br>

         <table align=center width=1000px>
      <td width=600px>
                      <center>
                          <span style="font-size:18px"><i>We further validate our method in a realistic robot setting using MuJoCo, a fast yet realistic physics simulator. Specifically, we use the Multi-agent MuJoCo benchmark, where the task is to learn a coordination policy for a Half Cheetah robot. The robot has 6 joints, where each joint is a different agent and distribution is achieved by enabling agent communication only with the adjacent joints, enforced by a ring graph topology.</i>
                    </center>
                    </td>
                </tr>
               <tr>
                 <td align=center width=900px>
                 <center>
                 <span style="font-size:20px">
                   <a href="./resources/halfcheetah.gif"><img src = "./resources/halfcheetah.gif" width="900px"></img></href></a>
                 </span>
                 </center>
                 </td>
             </tr>
            </table>
            <br><br>

        <hr>
         <center><h1>Code</h1></center>
            <table align=center width=800px>
              <tr><center> <br>
                <span style="font-size:28px">&nbsp;<a href='https://github.com/EduardoSebastianRodriguez/phMARL'>[github]</a>

                <span style="font-size:28px"></a></span>
              <br>
              </center></tr>
          </table>
            <br>
          <hr>

         <center><h1>Citation</h1></center>
            <table align=center width=800px>
              <tr><center> <br>
                          <span style="font-size:18px"><i>If you find our papers/code useful for your research, please cite our work as follows. </i>
              <br>
              </center></tr>
              <tr><left> <br>
                          <span style="font-size:18px">E. Sebastian, T. Duong, N. Atanasov, E. Montijano, C. Sagues. <a href='https://github.com/EduardoSebastianRodriguez/phMARL'>Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems</a>. Under review at IEEE Transactions on Robotics, 2024.

                          
              <br><br>
                              <code>@article{sebastian24phMARL,
                                    <br>
                                    author = {Eduardo Sebasti\'{a}n AND Thai Duong AND Nikolay Atanasov AND Eduardo Montijano AND Carlos Sag\"{u}\'{e}s},
                                  <br>
                                  title = {{Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems}},
                                  <br>
                                  journal = {arXiv preprint arXiv:2401.00212},
                                   <br>
                                    year = {2024}
                                    }
                              </code>
              </left></tr>
                  <br><br>
              
          </table>
            <br>
          <hr>

            <table align=center width=1100px>
                <tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                This work has been supported by ONR N00014-23-1-2353 and NSF CCF-2112665 (TILOS), by Spanish projects PID2021-125514NB-I00, PID2021-124137OB-I00 and TED2021-130224B-I00 funded by MCIN/AEI/10.13039/501100011033, by ERDF A way of making Europe and by the European Union NextGenerationEU/PRTR, DGA T45-23R, and Spanish grant FPU19-05700.
                <br>
            </left>
        </td>
        </tr>
        </table>
        <br><br>
</body>
</html>
